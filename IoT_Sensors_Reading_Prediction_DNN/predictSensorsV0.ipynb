{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d598eb17-ceb8-4652-8996-43c6df6c6fe3",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb43766a-feef-4a8a-b3e3-42eb21d4ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from IPython.display import Markdown\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de6f40e-10cc-4ba3-b4d1-ca7696f4de07",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792e139d-df36-4f6a-bf20-fd96246f3dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>water_precipitation</th>\n",
       "      <th>water_height</th>\n",
       "      <th>water_flow</th>\n",
       "      <th>water_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31.87</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11983985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.2</td>\n",
       "      <td>31.85</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11932800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.2</td>\n",
       "      <td>31.84</td>\n",
       "      <td>10.7</td>\n",
       "      <td>11907258.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   water_precipitation  water_height  water_flow  water_volume\n",
       "0                  0.0         31.87        12.2    11983985.0\n",
       "1                  2.2         31.85        11.0    11932800.0\n",
       "2                  2.2         31.84        10.7    11907258.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size :  (10227, 4)\n",
      "Missing Data :  0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('telva_dataset.csv', header=0,sep=';')\n",
    "data= data.drop('date',axis=1)\n",
    "data= data.drop('water_volume_variation',axis=1)\n",
    "display(data.head(3))\n",
    "print('size : ',data.shape)\n",
    "print('Missing Data : ',data.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5507444f-5bef-4a1e-8bd2-d9bc2cf865bd",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30dca83-76ce-4f66-bf8b-6639a883c815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8181, 3) (2046, 3) (8181,) (2046,)\n"
     ]
    }
   ],
   "source": [
    "X= data.drop('water_flow',axis=1)\n",
    "y=data['water_flow']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b31a1d-5e40-4a14-a5db-d02eda9fb404",
   "metadata": {},
   "source": [
    "# Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a419b21-b22e-4eb3-804a-195b3e55489c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f5154\">\n",
       "  <caption>Before normalization :</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f5154_level0_col0\" class=\"col_heading level0 col0\" >water_precipitation</th>\n",
       "      <th id=\"T_f5154_level0_col1\" class=\"col_heading level0 col1\" >water_height</th>\n",
       "      <th id=\"T_f5154_level0_col2\" class=\"col_heading level0 col2\" >water_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f5154_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_f5154_row0_col0\" class=\"data row0 col0\" >8181.00</td>\n",
       "      <td id=\"T_f5154_row0_col1\" class=\"data row0 col1\" >8181.00</td>\n",
       "      <td id=\"T_f5154_row0_col2\" class=\"data row0 col2\" >8181.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5154_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_f5154_row1_col0\" class=\"data row1 col0\" >3.13</td>\n",
       "      <td id=\"T_f5154_row1_col1\" class=\"data row1 col1\" >32.52</td>\n",
       "      <td id=\"T_f5154_row1_col2\" class=\"data row1 col2\" >14096957.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5154_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_f5154_row2_col0\" class=\"data row2 col0\" >6.90</td>\n",
       "      <td id=\"T_f5154_row2_col1\" class=\"data row2 col1\" >1.47</td>\n",
       "      <td id=\"T_f5154_row2_col2\" class=\"data row2 col2\" >3899472.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5154_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_f5154_row3_col0\" class=\"data row3 col0\" >0.00</td>\n",
       "      <td id=\"T_f5154_row3_col1\" class=\"data row3 col1\" >28.23</td>\n",
       "      <td id=\"T_f5154_row3_col2\" class=\"data row3 col2\" >4931058.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5154_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_f5154_row4_col0\" class=\"data row4 col0\" >0.00</td>\n",
       "      <td id=\"T_f5154_row4_col1\" class=\"data row4 col1\" >31.78</td>\n",
       "      <td id=\"T_f5154_row4_col2\" class=\"data row4 col2\" >11754698.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5154_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_f5154_row5_col0\" class=\"data row5 col0\" >0.00</td>\n",
       "      <td id=\"T_f5154_row5_col1\" class=\"data row5 col1\" >32.59</td>\n",
       "      <td id=\"T_f5154_row5_col2\" class=\"data row5 col2\" >13916032.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5154_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_f5154_row6_col0\" class=\"data row6 col0\" >3.00</td>\n",
       "      <td id=\"T_f5154_row6_col1\" class=\"data row6 col1\" >33.65</td>\n",
       "      <td id=\"T_f5154_row6_col2\" class=\"data row6 col2\" >17085322.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f5154_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_f5154_row7_col0\" class=\"data row7 col0\" >105.00</td>\n",
       "      <td id=\"T_f5154_row7_col1\" class=\"data row7 col1\" >35.15</td>\n",
       "      <td id=\"T_f5154_row7_col2\" class=\"data row7 col2\" >21687518.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x233f1f0afd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b1f7c\">\n",
       "  <caption>After normalization :</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b1f7c_level0_col0\" class=\"col_heading level0 col0\" >water_precipitation</th>\n",
       "      <th id=\"T_b1f7c_level0_col1\" class=\"col_heading level0 col1\" >water_height</th>\n",
       "      <th id=\"T_b1f7c_level0_col2\" class=\"col_heading level0 col2\" >water_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b1f7c_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_b1f7c_row0_col0\" class=\"data row0 col0\" >8181.00</td>\n",
       "      <td id=\"T_b1f7c_row0_col1\" class=\"data row0 col1\" >8181.00</td>\n",
       "      <td id=\"T_b1f7c_row0_col2\" class=\"data row0 col2\" >8181.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1f7c_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_b1f7c_row1_col0\" class=\"data row1 col0\" >-0.00</td>\n",
       "      <td id=\"T_b1f7c_row1_col1\" class=\"data row1 col1\" >-0.00</td>\n",
       "      <td id=\"T_b1f7c_row1_col2\" class=\"data row1 col2\" >-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1f7c_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_b1f7c_row2_col0\" class=\"data row2 col0\" >1.00</td>\n",
       "      <td id=\"T_b1f7c_row2_col1\" class=\"data row2 col1\" >1.00</td>\n",
       "      <td id=\"T_b1f7c_row2_col2\" class=\"data row2 col2\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1f7c_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_b1f7c_row3_col0\" class=\"data row3 col0\" >-0.45</td>\n",
       "      <td id=\"T_b1f7c_row3_col1\" class=\"data row3 col1\" >-2.93</td>\n",
       "      <td id=\"T_b1f7c_row3_col2\" class=\"data row3 col2\" >-2.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1f7c_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_b1f7c_row4_col0\" class=\"data row4 col0\" >-0.45</td>\n",
       "      <td id=\"T_b1f7c_row4_col1\" class=\"data row4 col1\" >-0.51</td>\n",
       "      <td id=\"T_b1f7c_row4_col2\" class=\"data row4 col2\" >-0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1f7c_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_b1f7c_row5_col0\" class=\"data row5 col0\" >-0.45</td>\n",
       "      <td id=\"T_b1f7c_row5_col1\" class=\"data row5 col1\" >0.05</td>\n",
       "      <td id=\"T_b1f7c_row5_col2\" class=\"data row5 col2\" >-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1f7c_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_b1f7c_row6_col0\" class=\"data row6 col0\" >-0.02</td>\n",
       "      <td id=\"T_b1f7c_row6_col1\" class=\"data row6 col1\" >0.77</td>\n",
       "      <td id=\"T_b1f7c_row6_col2\" class=\"data row6 col2\" >0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1f7c_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_b1f7c_row7_col0\" class=\"data row7 col0\" >14.77</td>\n",
       "      <td id=\"T_b1f7c_row7_col1\" class=\"data row7 col1\" >1.79</td>\n",
       "      <td id=\"T_b1f7c_row7_col2\" class=\"data row7 col2\" >1.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x233de054be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.describe().style.format(\"{0:.2f}\").set_caption(\"Before normalization :\"))\n",
    "\n",
    "mean = X_train.mean()\n",
    "std  = X_train.std()\n",
    "X_train = (X_train - mean) / std\n",
    "X_test  = (X_test  - mean) / std\n",
    "\n",
    "display(X_train.describe().style.format(\"{0:.2f}\").set_caption(\"After normalization :\"))\n",
    "\n",
    "# Convert ou DataFrame to numpy array\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_test,  y_test  = np.array(X_test),  np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5c4ee-5fef-4705-8fce-131825a45cde",
   "metadata": {},
   "source": [
    "# Build DNN model with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ea4b38-9b10-4c45-b3e4-527155a2b5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_n1 (Dense)            (None, 64)                256       \n",
      "                                                                 \n",
      " Dense_n2 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model_v1(shape):\n",
    " model = keras.models.Sequential()\n",
    " model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    " model.add(keras.layers.Dense(64, activation='relu', name='Dense_n1'))\n",
    " model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    " model.add(keras.layers.Dense(1, name='Output'))\n",
    " model.compile(optimizer='rmsprop', loss='mse', metrics=['mae','mse'])\n",
    " return model\n",
    "\n",
    "model=get_model_v1( (3,) )\n",
    "model.summary()\n",
    "os.makedirs('./run/models',   mode=0o750, exist_ok=True)\n",
    "save_dir = \"./run/models/best_model.h5\"\n",
    "savemodel_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir, verbose=0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd62ff-cf33-45c0-b772-82b30d4933e0",
   "metadata": {},
   "source": [
    "# Train and save the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c86324ea-b808-467b-bbb0-48da44ea776f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 35.1834 - mae: 3.5262 - mse: 35.1834 - val_loss: 39.5787 - val_mae: 3.6839 - val_mse: 39.5787\n",
      "Epoch 2/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.7522 - mae: 3.4773 - mse: 32.7522 - val_loss: 39.2456 - val_mae: 3.4379 - val_mse: 39.2456\n",
      "Epoch 3/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.5404 - mae: 3.4578 - mse: 32.5404 - val_loss: 39.2860 - val_mae: 3.3713 - val_mse: 39.2860\n",
      "Epoch 4/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.4673 - mae: 3.4540 - mse: 32.4673 - val_loss: 38.6766 - val_mae: 3.5984 - val_mse: 38.6766\n",
      "Epoch 5/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3959 - mae: 3.4425 - mse: 32.3959 - val_loss: 38.8819 - val_mae: 3.6653 - val_mse: 38.8819\n",
      "Epoch 6/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3767 - mae: 3.4475 - mse: 32.3767 - val_loss: 38.5615 - val_mae: 3.7070 - val_mse: 38.5615\n",
      "Epoch 7/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.4513 - mae: 3.4464 - mse: 32.4513 - val_loss: 38.8362 - val_mae: 3.5523 - val_mse: 38.8362\n",
      "Epoch 8/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3416 - mae: 3.4496 - mse: 32.3416 - val_loss: 38.6554 - val_mae: 3.4698 - val_mse: 38.6554\n",
      "Epoch 9/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2706 - mae: 3.4462 - mse: 32.2706 - val_loss: 39.3110 - val_mae: 3.7210 - val_mse: 39.3110\n",
      "Epoch 10/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3682 - mae: 3.4495 - mse: 32.3682 - val_loss: 38.6146 - val_mae: 3.5798 - val_mse: 38.6146\n",
      "Epoch 11/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3144 - mae: 3.4416 - mse: 32.3144 - val_loss: 39.0217 - val_mae: 3.7186 - val_mse: 39.0217\n",
      "Epoch 12/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3711 - mae: 3.4475 - mse: 32.3711 - val_loss: 39.0123 - val_mae: 3.4280 - val_mse: 39.0123\n",
      "Epoch 13/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3339 - mae: 3.4296 - mse: 32.3339 - val_loss: 39.1877 - val_mae: 3.3505 - val_mse: 39.1877\n",
      "Epoch 14/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2877 - mae: 3.4324 - mse: 32.2877 - val_loss: 38.9999 - val_mae: 3.5072 - val_mse: 38.9999\n",
      "Epoch 15/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3247 - mae: 3.4363 - mse: 32.3247 - val_loss: 38.7784 - val_mae: 3.4840 - val_mse: 38.7784\n",
      "Epoch 16/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2029 - mae: 3.4472 - mse: 32.2029 - val_loss: 38.3719 - val_mae: 3.5989 - val_mse: 38.3719\n",
      "Epoch 17/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2941 - mae: 3.4371 - mse: 32.2941 - val_loss: 38.4648 - val_mae: 3.5823 - val_mse: 38.4648\n",
      "Epoch 18/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2829 - mae: 3.4330 - mse: 32.2829 - val_loss: 38.8416 - val_mae: 3.5151 - val_mse: 38.8416\n",
      "Epoch 19/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2520 - mae: 3.4316 - mse: 32.2520 - val_loss: 38.5233 - val_mae: 3.7124 - val_mse: 38.5233\n",
      "Epoch 20/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2954 - mae: 3.4319 - mse: 32.2954 - val_loss: 38.5927 - val_mae: 3.5897 - val_mse: 38.5927\n",
      "Epoch 21/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2574 - mae: 3.4378 - mse: 32.2574 - val_loss: 38.7862 - val_mae: 3.4943 - val_mse: 38.7862\n",
      "Epoch 22/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2107 - mae: 3.4312 - mse: 32.2107 - val_loss: 39.2266 - val_mae: 3.4454 - val_mse: 39.2266\n",
      "Epoch 23/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2057 - mae: 3.4310 - mse: 32.2057 - val_loss: 38.8626 - val_mae: 3.4250 - val_mse: 38.8626\n",
      "Epoch 24/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2934 - mae: 3.4224 - mse: 32.2934 - val_loss: 38.4901 - val_mae: 3.7398 - val_mse: 38.4901\n",
      "Epoch 25/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2908 - mae: 3.4348 - mse: 32.2908 - val_loss: 38.5323 - val_mae: 3.7183 - val_mse: 38.5324\n",
      "Epoch 26/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2250 - mae: 3.4332 - mse: 32.2250 - val_loss: 38.6364 - val_mae: 3.8532 - val_mse: 38.6364\n",
      "Epoch 27/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2148 - mae: 3.4267 - mse: 32.2148 - val_loss: 39.2026 - val_mae: 3.4219 - val_mse: 39.2026\n",
      "Epoch 28/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2523 - mae: 3.4234 - mse: 32.2523 - val_loss: 38.9587 - val_mae: 3.4828 - val_mse: 38.9587\n",
      "Epoch 29/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2107 - mae: 3.4147 - mse: 32.2107 - val_loss: 38.7987 - val_mae: 3.5643 - val_mse: 38.7987\n",
      "Epoch 30/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2263 - mae: 3.4086 - mse: 32.2263 - val_loss: 39.0458 - val_mae: 3.4982 - val_mse: 39.0458\n",
      "Epoch 31/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2184 - mae: 3.4216 - mse: 32.2184 - val_loss: 39.7524 - val_mae: 3.4410 - val_mse: 39.7524\n",
      "Epoch 32/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2562 - mae: 3.4161 - mse: 32.2562 - val_loss: 38.8110 - val_mae: 3.4846 - val_mse: 38.8110\n",
      "Epoch 33/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1509 - mae: 3.4156 - mse: 32.1509 - val_loss: 39.5639 - val_mae: 3.5247 - val_mse: 39.5639\n",
      "Epoch 34/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1767 - mae: 3.4195 - mse: 32.1767 - val_loss: 38.9417 - val_mae: 3.7574 - val_mse: 38.9417\n",
      "Epoch 35/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1940 - mae: 3.4385 - mse: 32.1940 - val_loss: 39.1103 - val_mae: 3.4063 - val_mse: 39.1103\n",
      "Epoch 36/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2480 - mae: 3.4227 - mse: 32.2480 - val_loss: 38.6175 - val_mae: 3.6003 - val_mse: 38.6175\n",
      "Epoch 37/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1768 - mae: 3.4299 - mse: 32.1768 - val_loss: 39.1365 - val_mae: 3.5898 - val_mse: 39.1365\n",
      "Epoch 38/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2092 - mae: 3.4192 - mse: 32.2092 - val_loss: 38.5369 - val_mae: 3.6879 - val_mse: 38.5369\n",
      "Epoch 39/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1963 - mae: 3.4231 - mse: 32.1963 - val_loss: 38.8565 - val_mae: 3.4761 - val_mse: 38.8565\n",
      "Epoch 40/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1893 - mae: 3.3998 - mse: 32.1893 - val_loss: 39.5043 - val_mae: 3.4080 - val_mse: 39.5043\n",
      "Epoch 41/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.1811 - mae: 3.4111 - mse: 32.1811 - val_loss: 38.8175 - val_mae: 3.4650 - val_mse: 38.8175\n",
      "Epoch 42/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2233 - mae: 3.4097 - mse: 32.2233 - val_loss: 38.4551 - val_mae: 3.5268 - val_mse: 38.4551\n",
      "Epoch 43/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1993 - mae: 3.4103 - mse: 32.1993 - val_loss: 38.7661 - val_mae: 3.4522 - val_mse: 38.7661\n",
      "Epoch 44/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1994 - mae: 3.4200 - mse: 32.1994 - val_loss: 38.2448 - val_mae: 3.6200 - val_mse: 38.2448\n",
      "Epoch 45/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1837 - mae: 3.4177 - mse: 32.1837 - val_loss: 38.4622 - val_mae: 3.6373 - val_mse: 38.4622\n",
      "Epoch 46/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.1237 - mae: 3.4238 - mse: 32.1237 - val_loss: 38.6922 - val_mae: 3.4895 - val_mse: 38.6922\n",
      "Epoch 47/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0916 - mae: 3.4173 - mse: 32.0916 - val_loss: 39.0295 - val_mae: 3.3962 - val_mse: 39.0295\n",
      "Epoch 48/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2017 - mae: 3.4151 - mse: 32.2017 - val_loss: 38.6269 - val_mae: 3.4920 - val_mse: 38.6269\n",
      "Epoch 49/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1453 - mae: 3.4138 - mse: 32.1453 - val_loss: 38.6563 - val_mae: 3.5775 - val_mse: 38.6563\n",
      "Epoch 50/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1625 - mae: 3.4196 - mse: 32.1625 - val_loss: 38.4340 - val_mae: 3.7142 - val_mse: 38.4340\n",
      "Epoch 51/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2026 - mae: 3.4155 - mse: 32.2026 - val_loss: 38.6717 - val_mae: 3.5741 - val_mse: 38.6717\n",
      "Epoch 52/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1929 - mae: 3.4189 - mse: 32.1929 - val_loss: 38.5418 - val_mae: 3.5001 - val_mse: 38.5418\n",
      "Epoch 53/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0718 - mae: 3.4231 - mse: 32.0718 - val_loss: 39.2111 - val_mae: 3.4762 - val_mse: 39.2111\n",
      "Epoch 54/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1600 - mae: 3.4099 - mse: 32.1600 - val_loss: 38.5694 - val_mae: 3.5629 - val_mse: 38.5694\n",
      "Epoch 55/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1927 - mae: 3.4232 - mse: 32.1927 - val_loss: 38.7337 - val_mae: 3.5424 - val_mse: 38.7337\n",
      "Epoch 56/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1644 - mae: 3.4221 - mse: 32.1644 - val_loss: 38.5703 - val_mae: 3.6281 - val_mse: 38.5703\n",
      "Epoch 57/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1510 - mae: 3.4268 - mse: 32.1510 - val_loss: 38.6905 - val_mae: 3.5060 - val_mse: 38.6905\n",
      "Epoch 58/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0998 - mae: 3.4217 - mse: 32.0998 - val_loss: 38.5832 - val_mae: 3.5444 - val_mse: 38.5832\n",
      "Epoch 59/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0925 - mae: 3.4150 - mse: 32.0925 - val_loss: 39.2774 - val_mae: 3.3942 - val_mse: 39.2774\n",
      "Epoch 60/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1508 - mae: 3.4129 - mse: 32.1508 - val_loss: 38.5552 - val_mae: 3.6702 - val_mse: 38.5552\n",
      "Epoch 61/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2010 - mae: 3.4104 - mse: 32.2010 - val_loss: 38.6772 - val_mae: 3.7506 - val_mse: 38.6772\n",
      "Epoch 62/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1540 - mae: 3.4191 - mse: 32.1540 - val_loss: 38.9375 - val_mae: 3.5712 - val_mse: 38.9375\n",
      "Epoch 63/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0861 - mae: 3.4234 - mse: 32.0861 - val_loss: 39.2417 - val_mae: 3.4527 - val_mse: 39.2417\n",
      "Epoch 64/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0994 - mae: 3.4280 - mse: 32.0994 - val_loss: 38.8277 - val_mae: 3.6587 - val_mse: 38.8277\n",
      "Epoch 65/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1999 - mae: 3.4211 - mse: 32.1999 - val_loss: 39.0913 - val_mae: 3.3699 - val_mse: 39.0913\n",
      "Epoch 66/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1138 - mae: 3.4129 - mse: 32.1138 - val_loss: 38.5153 - val_mae: 3.6999 - val_mse: 38.5153\n",
      "Epoch 67/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0644 - mae: 3.4204 - mse: 32.0644 - val_loss: 38.4060 - val_mae: 3.6369 - val_mse: 38.4060\n",
      "Epoch 68/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0857 - mae: 3.4166 - mse: 32.0857 - val_loss: 38.9758 - val_mae: 3.5718 - val_mse: 38.9758\n",
      "Epoch 69/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1013 - mae: 3.4108 - mse: 32.1013 - val_loss: 39.1808 - val_mae: 3.5625 - val_mse: 39.1808\n",
      "Epoch 70/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.1062 - mae: 3.4118 - mse: 32.1062 - val_loss: 38.4403 - val_mae: 3.6632 - val_mse: 38.4403\n",
      "Epoch 71/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2017 - mae: 3.4178 - mse: 32.2017 - val_loss: 39.3154 - val_mae: 3.3588 - val_mse: 39.3154\n",
      "Epoch 72/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0915 - mae: 3.4162 - mse: 32.0915 - val_loss: 39.5245 - val_mae: 3.3884 - val_mse: 39.5245\n",
      "Epoch 73/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2546 - mae: 3.4270 - mse: 32.2546 - val_loss: 38.5826 - val_mae: 3.4633 - val_mse: 38.5826\n",
      "Epoch 74/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2082 - mae: 3.4082 - mse: 32.2082 - val_loss: 38.9450 - val_mae: 3.4874 - val_mse: 38.9450\n",
      "Epoch 75/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1673 - mae: 3.4022 - mse: 32.1673 - val_loss: 38.6752 - val_mae: 3.6991 - val_mse: 38.6752\n",
      "Epoch 76/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1131 - mae: 3.4219 - mse: 32.1131 - val_loss: 38.7402 - val_mae: 3.5575 - val_mse: 38.7402\n",
      "Epoch 77/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0863 - mae: 3.4234 - mse: 32.0863 - val_loss: 39.1193 - val_mae: 3.3874 - val_mse: 39.1193\n",
      "Epoch 78/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1139 - mae: 3.4137 - mse: 32.1139 - val_loss: 38.9251 - val_mae: 3.5435 - val_mse: 38.9251\n",
      "Epoch 79/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1472 - mae: 3.4092 - mse: 32.1472 - val_loss: 39.1056 - val_mae: 3.4628 - val_mse: 39.1056\n",
      "Epoch 80/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1096 - mae: 3.4078 - mse: 32.1096 - val_loss: 38.5155 - val_mae: 3.6668 - val_mse: 38.5155\n",
      "Epoch 81/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1205 - mae: 3.4155 - mse: 32.1205 - val_loss: 38.7120 - val_mae: 3.6743 - val_mse: 38.7120\n",
      "Epoch 82/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1047 - mae: 3.4101 - mse: 32.1047 - val_loss: 38.8078 - val_mae: 3.5260 - val_mse: 38.8078\n",
      "Epoch 83/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1989 - mae: 3.4167 - mse: 32.1989 - val_loss: 38.8769 - val_mae: 3.4693 - val_mse: 38.8769\n",
      "Epoch 84/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1798 - mae: 3.4077 - mse: 32.1798 - val_loss: 38.5137 - val_mae: 3.5653 - val_mse: 38.5137\n",
      "Epoch 85/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1259 - mae: 3.4093 - mse: 32.1259 - val_loss: 38.4362 - val_mae: 3.7639 - val_mse: 38.4362\n",
      "Epoch 86/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1140 - mae: 3.4163 - mse: 32.1140 - val_loss: 39.5258 - val_mae: 3.3937 - val_mse: 39.5258\n",
      "Epoch 87/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1189 - mae: 3.3966 - mse: 32.1189 - val_loss: 38.4407 - val_mae: 3.6787 - val_mse: 38.4407\n",
      "Epoch 88/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1298 - mae: 3.4156 - mse: 32.1298 - val_loss: 38.6291 - val_mae: 3.8902 - val_mse: 38.6291\n",
      "Epoch 89/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1337 - mae: 3.4230 - mse: 32.1337 - val_loss: 39.2614 - val_mae: 3.3708 - val_mse: 39.2614\n",
      "Epoch 90/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.0830 - mae: 3.4059 - mse: 32.0830 - val_loss: 38.3486 - val_mae: 3.7197 - val_mse: 38.3486\n",
      "Epoch 91/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0687 - mae: 3.4077 - mse: 32.0687 - val_loss: 39.7708 - val_mae: 3.3527 - val_mse: 39.7708\n",
      "Epoch 92/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1760 - mae: 3.4111 - mse: 32.1760 - val_loss: 38.5677 - val_mae: 3.5378 - val_mse: 38.5677\n",
      "Epoch 93/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1048 - mae: 3.4049 - mse: 32.1048 - val_loss: 38.6362 - val_mae: 3.7152 - val_mse: 38.6362\n",
      "Epoch 94/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.1043 - mae: 3.4209 - mse: 32.1043 - val_loss: 38.9224 - val_mae: 3.3467 - val_mse: 38.9224\n",
      "Epoch 95/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0960 - mae: 3.4037 - mse: 32.0960 - val_loss: 39.9590 - val_mae: 3.2378 - val_mse: 39.9590\n",
      "Epoch 96/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0921 - mae: 3.4107 - mse: 32.0921 - val_loss: 39.3345 - val_mae: 3.3452 - val_mse: 39.3345\n",
      "Epoch 97/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1554 - mae: 3.4008 - mse: 32.1554 - val_loss: 38.4808 - val_mae: 3.6280 - val_mse: 38.4808\n",
      "Epoch 98/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1019 - mae: 3.4099 - mse: 32.1019 - val_loss: 38.9007 - val_mae: 3.6363 - val_mse: 38.9007\n",
      "Epoch 99/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1634 - mae: 3.4194 - mse: 32.1634 - val_loss: 38.6846 - val_mae: 3.5170 - val_mse: 38.6846\n",
      "Epoch 100/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1583 - mae: 3.4126 - mse: 32.1583 - val_loss: 38.9221 - val_mae: 3.6129 - val_mse: 38.9221\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs= 100, batch_size= 10, validation_data = (X_test, y_test), callbacks= [savemodel_callback])\n",
    "model.save('my_model.h5')\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d782288-50f3-4094-b696-7a93d589f9cc",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d28eb09c-7978-476e-b164-c8602915819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test / loss      : 38.9221\n",
      "x_test / mae       : 3.6129\n",
      "x_test / mse       : 38.9221\n",
      "min( val_mae ) : 3.2378\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))\n",
    "print('x_test / mse       : {:5.4f}'.format(score[2]))\n",
    "print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470254ec-9c90-4ff8-ad8b-467fe7b8f615",
   "metadata": {},
   "source": [
    "# Make and display prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "408906e9-3fcf-428b-87f0-c456a3ef9b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 1ms/step\n",
      "[[9.031281 ]\n",
      " [2.9480765]\n",
      " [4.2165117]\n",
      " ...\n",
      " [5.1594534]\n",
      " [8.132318 ]\n",
      " [4.221397 ]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a83fc7f-d551-4257-951c-c742ee3405fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture    Prediction   Real   Delta\n",
      "000        9.03       32.97      +23.94 \n",
      "001        2.95       1.54      -1.41 \n",
      "002        4.22       2.5      -1.72 \n",
      "003        8.93       2.13      -6.80 \n",
      "004        6.06       1.7      -4.36 \n",
      "005        4.08       6.19      +2.11 \n",
      "006        6.16       1.71      -4.45 \n",
      "007        3.38       1.61      -1.77 \n",
      "008        4.59       1.85      -2.74 \n",
      "009        3.14       1.48      -1.66 \n",
      "010        5.08       1.37      -3.71 \n",
      "011        4.22       14.0      +9.78 \n",
      "012        4.64       2.19      -2.45 \n",
      "013        5.10       11.38      +6.28 \n",
      "014        7.41       12.24      +4.83 \n",
      "015        5.74       7.82      +2.08 \n",
      "016        3.58       1.37      -2.21 \n",
      "017        5.77       4.08      -1.69 \n",
      "018        3.71       2.19      -1.52 \n",
      "019        7.33       1.93      -5.40 \n",
      "020        3.81       1.54      -2.27 \n",
      "021        4.36       2.64      -1.72 \n",
      "022        3.58       1.57      -2.01 \n",
      "023        3.98       2.41      -1.57 \n",
      "024        4.51       6.85      +2.34 \n",
      "025        4.06       2.97      -1.09 \n",
      "026        5.80       5.1      -0.70 \n",
      "027        4.36       2.0      -2.36 \n",
      "028        3.90       1.97      -1.93 \n",
      "029        6.33       2.0      -4.33 \n",
      "030        6.21       1.44      -4.77 \n",
      "031        8.97       1.42      -7.55 \n",
      "032        4.56       3.71      -0.85 \n",
      "033        4.92       3.5      -1.42 \n",
      "034        8.57       7.98      -0.59 \n",
      "035        3.68       5.1      +1.42 \n",
      "036        3.46       1.59      -1.87 \n",
      "037        5.96       1.71      -4.25 \n",
      "038        4.89       20.19      +15.30 \n",
      "039        6.64       3.74      -2.90 \n",
      "040        9.21       22.44      +13.23 \n",
      "041        5.18       9.66      +4.48 \n",
      "042        4.05       2.1      -1.95 \n",
      "043        4.15       1.57      -2.58 \n",
      "044        2.89       1.66      -1.23 \n",
      "045        7.66       2.83      -4.83 \n",
      "046        3.37       5.3      +1.93 \n",
      "047        3.05       9.81      +6.76 \n",
      "048        3.71       5.1      +1.39 \n",
      "049        6.16       2.12      -4.04 \n",
      "050        3.08       1.83      -1.25 \n",
      "051        5.09       2.61      -2.48 \n",
      "052        3.65       1.56      -2.09 \n",
      "053        4.60       2.29      -2.31 \n",
      "054        4.94       5.11      +0.17 \n",
      "055        4.76       1.63      -3.13 \n",
      "056        8.22       6.96      -1.26 \n",
      "057        3.32       8.95      +5.63 \n",
      "058        4.86       10.9      +6.04 \n",
      "059        9.00       4.67      -4.33 \n",
      "060        5.75       2.2      -3.55 \n",
      "061        8.92       6.14      -2.78 \n",
      "062        4.52       6.91      +2.39 \n",
      "063        4.46       2.06      -2.40 \n",
      "064        7.04       1.62      -5.42 \n",
      "065        3.20       8.13      +4.93 \n",
      "066        4.21       2.36      -1.85 \n",
      "067        4.62       22.77      +18.15 \n",
      "068        6.25       1.33      -4.92 \n",
      "069        4.05       2.92      -1.13 \n",
      "070        4.95       1.3      -3.65 \n",
      "071        3.96       11.8      +7.84 \n",
      "072        6.80       2.65      -4.15 \n",
      "073        3.95       1.51      -2.44 \n",
      "074        4.66       1.42      -3.24 \n",
      "075        4.56       3.79      -0.77 \n",
      "076        3.17       1.47      -1.70 \n",
      "077        4.67       20.61      +15.94 \n",
      "078        6.27       4.62      -1.65 \n",
      "079        6.81       31.64      +24.83 \n",
      "080        4.65       4.08      -0.57 \n",
      "081        4.25       7.2      +2.95 \n",
      "082        4.83       5.6      +0.77 \n",
      "083        8.66       4.67      -3.99 \n",
      "084        4.19       2.1      -2.09 \n",
      "085        4.26       3.3      -0.96 \n",
      "086        4.34       2.33      -2.01 \n",
      "087        5.19       8.24      +3.05 \n",
      "088        4.10       2.92      -1.18 \n",
      "089        3.11       1.57      -1.54 \n",
      "090        6.59       1.73      -4.86 \n",
      "091        4.68       2.22      -2.46 \n",
      "092        6.64       2.2      -4.44 \n",
      "093        6.75       11.45      +4.70 \n",
      "094        4.37       1.31      -3.06 \n",
      "095        7.85       7.83      -0.02 \n",
      "096        4.15       2.85      -1.30 \n",
      "097        3.45       1.59      -1.86 \n",
      "098        3.52       1.58      -1.94 \n",
      "099        4.53       7.24      +2.71 \n",
      "100        3.74       3.82      +0.08 \n",
      "101        7.16       9.36      +2.20 \n",
      "102        6.42       2.67      -3.75 \n",
      "103        4.27       1.32      -2.95 \n",
      "104        6.04       4.3      -1.74 \n",
      "105        3.89       6.57      +2.68 \n",
      "106        2.89       2.15      -0.74 \n",
      "107        3.66       1.49      -2.17 \n",
      "108        8.11       2.28      -5.83 \n",
      "109        4.10       4.3      +0.20 \n",
      "110        5.18       4.96      -0.22 \n",
      "111        9.05       13.1      +4.05 \n",
      "112        5.62       3.12      -2.50 \n",
      "113        6.34       2.71      -3.63 \n",
      "114        6.13       13.48      +7.35 \n",
      "115        4.21       4.16      -0.05 \n",
      "116        6.21       2.0      -4.21 \n",
      "117        3.58       3.34      -0.24 \n",
      "118        4.29       5.26      +0.97 \n",
      "119        3.84       1.77      -2.07 \n",
      "120        5.16       4.07      -1.09 \n",
      "121        4.84       1.76      -3.08 \n",
      "122        3.42       1.64      -1.78 \n",
      "123        4.15       1.5      -2.65 \n",
      "124        4.22       2.1      -2.12 \n",
      "125        5.91       9.21      +3.30 \n",
      "126        4.65       2.23      -2.42 \n",
      "127        4.54       1.42      -3.12 \n",
      "128        2.67       2.75      +0.08 \n",
      "129        3.86       2.43      -1.43 \n",
      "130        5.04       3.8      -1.24 \n",
      "131        5.68       1.81      -3.87 \n",
      "132        6.30       6.54      +0.24 \n",
      "133        3.12       8.26      +5.14 \n",
      "134        2.80       2.15      -0.65 \n",
      "135        3.31       1.3      -2.01 \n",
      "136        5.09       2.5      -2.59 \n",
      "137        3.30       1.62      -1.68 \n",
      "138        7.49       5.0      -2.49 \n",
      "139        6.04       8.9      +2.86 \n",
      "140        4.07       4.83      +0.76 \n",
      "141        3.72       1.55      -2.17 \n",
      "142        3.66       5.99      +2.33 \n",
      "143        5.80       11.63      +5.83 \n",
      "144        4.52       2.85      -1.67 \n",
      "145        8.63       5.17      -3.46 \n",
      "146        3.81       1.9      -1.91 \n",
      "147        5.15       3.79      -1.36 \n",
      "148        4.76       16.7      +11.94 \n",
      "149        4.21       1.27      -2.94 \n",
      "150        3.98       2.68      -1.30 \n",
      "151        5.38       1.7      -3.68 \n",
      "152        8.15       17.18      +9.03 \n",
      "153        4.41       27.0      +22.59 \n",
      "154        2.71       3.48      +0.77 \n",
      "155        6.87       14.54      +7.67 \n",
      "156        4.23       2.5      -1.73 \n",
      "157        5.45       3.55      -1.90 \n",
      "158        5.68       2.1      -3.58 \n",
      "159        3.14       1.48      -1.66 \n",
      "160        5.80       1.7      -4.10 \n",
      "161        4.20       2.04      -2.16 \n",
      "162        3.06       1.82      -1.24 \n",
      "163        3.63       3.52      -0.11 \n",
      "164        8.55       15.1      +6.55 \n",
      "165        3.97       30.4      +26.43 \n",
      "166        5.73       10.22      +4.49 \n",
      "167        4.83       5.81      +0.98 \n",
      "168        4.22       1.2      -3.02 \n",
      "169        3.67       1.56      -2.11 \n",
      "170        4.90       2.28      -2.62 \n",
      "171        4.21       14.2      +9.99 \n",
      "172        4.22       11.11      +6.89 \n",
      "173        4.23       1.3      -2.93 \n",
      "174        3.10       1.5      -1.60 \n",
      "175        3.46       8.6      +5.14 \n",
      "176        8.00       12.68      +4.68 \n",
      "177        7.28       10.29      +3.01 \n",
      "178        3.10       1.48      -1.62 \n",
      "179        5.36       15.4      +10.04 \n",
      "180        3.93       1.79      -2.14 \n",
      "181        9.01       9.7      +0.69 \n",
      "182        3.98       2.3      -1.68 \n",
      "183        4.54       2.62      -1.92 \n",
      "184        6.29       10.9      +4.61 \n",
      "185        5.50       14.0      +8.50 \n",
      "186        6.29       26.12      +19.83 \n",
      "187        3.12       2.64      -0.48 \n",
      "188        4.22       3.24      -0.98 \n",
      "189        3.71       15.18      +11.47 \n",
      "190        4.60       8.37      +3.77 \n",
      "191        3.87       3.16      -0.71 \n",
      "192        3.38       6.29      +2.91 \n",
      "193        5.60       7.09      +1.49 \n",
      "194        3.03       1.56      -1.47 \n",
      "195        8.09       12.02      +3.93 \n",
      "196        3.68       2.2      -1.48 \n",
      "197        4.27       4.58      +0.31 \n",
      "198        3.00       1.49      -1.51 \n",
      "199        4.15       2.1      -2.05 \n"
     ]
    }
   ],
   "source": [
    "n = 200\n",
    "ii = np.random.randint(1,len(X_test),n)\n",
    "x_sample = X_test[ii]\n",
    "y_sample = y_test[ii]\n",
    "print('Capture    Prediction   Real   Delta')\n",
    "for i in range(n):\n",
    "    pred   = y_pred[i][0]\n",
    "    real   = y_test[i]\n",
    "    delta  = real-pred\n",
    "    print(f'{i:03d}        {pred:.2f}       {real}      {delta:+.2f} ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
