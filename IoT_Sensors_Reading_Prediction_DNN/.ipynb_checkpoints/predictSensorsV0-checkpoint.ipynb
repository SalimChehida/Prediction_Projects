{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d598eb17-ceb8-4652-8996-43c6df6c6fe3",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb43766a-feef-4a8a-b3e3-42eb21d4ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from IPython.display import Markdown\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de6f40e-10cc-4ba3-b4d1-ca7696f4de07",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "792e139d-df36-4f6a-bf20-fd96246f3dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>water_precipitation</th>\n",
       "      <th>water_height</th>\n",
       "      <th>water_flow</th>\n",
       "      <th>water_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31.87</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11983985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.2</td>\n",
       "      <td>31.85</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11932800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.2</td>\n",
       "      <td>31.84</td>\n",
       "      <td>10.7</td>\n",
       "      <td>11907258.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   water_precipitation  water_height  water_flow  water_volume\n",
       "0                  0.0         31.87        12.2    11983985.0\n",
       "1                  2.2         31.85        11.0    11932800.0\n",
       "2                  2.2         31.84        10.7    11907258.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size :  (10227, 4)\n",
      "Missing Data :  0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('telva_dataset.csv', header=0,sep=';')\n",
    "data= data.drop('date',axis=1)\n",
    "data= data.drop('water_volume_variation',axis=1)\n",
    "display(data.head(3))\n",
    "print('size : ',data.shape)\n",
    "print('Missing Data : ',data.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5507444f-5bef-4a1e-8bd2-d9bc2cf865bd",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e30dca83-76ce-4f66-bf8b-6639a883c815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8181, 3) (2046, 3) (8181,) (2046,)\n"
     ]
    }
   ],
   "source": [
    "X= data.drop('water_flow',axis=1)\n",
    "y=data['water_flow']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b31a1d-5e40-4a14-a5db-d02eda9fb404",
   "metadata": {},
   "source": [
    "# Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a419b21-b22e-4eb3-804a-195b3e55489c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_d691d\">\n",
       "  <caption>Before normalization :</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d691d_level0_col0\" class=\"col_heading level0 col0\" >water_precipitation</th>\n",
       "      <th id=\"T_d691d_level0_col1\" class=\"col_heading level0 col1\" >water_height</th>\n",
       "      <th id=\"T_d691d_level0_col2\" class=\"col_heading level0 col2\" >water_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d691d_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_d691d_row0_col0\" class=\"data row0 col0\" >8181.00</td>\n",
       "      <td id=\"T_d691d_row0_col1\" class=\"data row0 col1\" >8181.00</td>\n",
       "      <td id=\"T_d691d_row0_col2\" class=\"data row0 col2\" >8181.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d691d_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_d691d_row1_col0\" class=\"data row1 col0\" >3.13</td>\n",
       "      <td id=\"T_d691d_row1_col1\" class=\"data row1 col1\" >32.52</td>\n",
       "      <td id=\"T_d691d_row1_col2\" class=\"data row1 col2\" >14096957.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d691d_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_d691d_row2_col0\" class=\"data row2 col0\" >6.90</td>\n",
       "      <td id=\"T_d691d_row2_col1\" class=\"data row2 col1\" >1.47</td>\n",
       "      <td id=\"T_d691d_row2_col2\" class=\"data row2 col2\" >3899472.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d691d_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_d691d_row3_col0\" class=\"data row3 col0\" >0.00</td>\n",
       "      <td id=\"T_d691d_row3_col1\" class=\"data row3 col1\" >28.23</td>\n",
       "      <td id=\"T_d691d_row3_col2\" class=\"data row3 col2\" >4931058.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d691d_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_d691d_row4_col0\" class=\"data row4 col0\" >0.00</td>\n",
       "      <td id=\"T_d691d_row4_col1\" class=\"data row4 col1\" >31.78</td>\n",
       "      <td id=\"T_d691d_row4_col2\" class=\"data row4 col2\" >11754698.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d691d_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_d691d_row5_col0\" class=\"data row5 col0\" >0.00</td>\n",
       "      <td id=\"T_d691d_row5_col1\" class=\"data row5 col1\" >32.59</td>\n",
       "      <td id=\"T_d691d_row5_col2\" class=\"data row5 col2\" >13916032.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d691d_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_d691d_row6_col0\" class=\"data row6 col0\" >3.00</td>\n",
       "      <td id=\"T_d691d_row6_col1\" class=\"data row6 col1\" >33.65</td>\n",
       "      <td id=\"T_d691d_row6_col2\" class=\"data row6 col2\" >17085322.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d691d_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_d691d_row7_col0\" class=\"data row7 col0\" >105.00</td>\n",
       "      <td id=\"T_d691d_row7_col1\" class=\"data row7 col1\" >35.15</td>\n",
       "      <td id=\"T_d691d_row7_col2\" class=\"data row7 col2\" >21687518.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x240ecbc9910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1603d\">\n",
       "  <caption>After normalization :</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1603d_level0_col0\" class=\"col_heading level0 col0\" >water_precipitation</th>\n",
       "      <th id=\"T_1603d_level0_col1\" class=\"col_heading level0 col1\" >water_height</th>\n",
       "      <th id=\"T_1603d_level0_col2\" class=\"col_heading level0 col2\" >water_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1603d_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "      <td id=\"T_1603d_row0_col0\" class=\"data row0 col0\" >8181.00</td>\n",
       "      <td id=\"T_1603d_row0_col1\" class=\"data row0 col1\" >8181.00</td>\n",
       "      <td id=\"T_1603d_row0_col2\" class=\"data row0 col2\" >8181.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1603d_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "      <td id=\"T_1603d_row1_col0\" class=\"data row1 col0\" >-0.00</td>\n",
       "      <td id=\"T_1603d_row1_col1\" class=\"data row1 col1\" >-0.00</td>\n",
       "      <td id=\"T_1603d_row1_col2\" class=\"data row1 col2\" >-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1603d_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "      <td id=\"T_1603d_row2_col0\" class=\"data row2 col0\" >1.00</td>\n",
       "      <td id=\"T_1603d_row2_col1\" class=\"data row2 col1\" >1.00</td>\n",
       "      <td id=\"T_1603d_row2_col2\" class=\"data row2 col2\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1603d_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "      <td id=\"T_1603d_row3_col0\" class=\"data row3 col0\" >-0.45</td>\n",
       "      <td id=\"T_1603d_row3_col1\" class=\"data row3 col1\" >-2.93</td>\n",
       "      <td id=\"T_1603d_row3_col2\" class=\"data row3 col2\" >-2.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1603d_level0_row4\" class=\"row_heading level0 row4\" >25%</th>\n",
       "      <td id=\"T_1603d_row4_col0\" class=\"data row4 col0\" >-0.45</td>\n",
       "      <td id=\"T_1603d_row4_col1\" class=\"data row4 col1\" >-0.51</td>\n",
       "      <td id=\"T_1603d_row4_col2\" class=\"data row4 col2\" >-0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1603d_level0_row5\" class=\"row_heading level0 row5\" >50%</th>\n",
       "      <td id=\"T_1603d_row5_col0\" class=\"data row5 col0\" >-0.45</td>\n",
       "      <td id=\"T_1603d_row5_col1\" class=\"data row5 col1\" >0.05</td>\n",
       "      <td id=\"T_1603d_row5_col2\" class=\"data row5 col2\" >-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1603d_level0_row6\" class=\"row_heading level0 row6\" >75%</th>\n",
       "      <td id=\"T_1603d_row6_col0\" class=\"data row6 col0\" >-0.02</td>\n",
       "      <td id=\"T_1603d_row6_col1\" class=\"data row6 col1\" >0.77</td>\n",
       "      <td id=\"T_1603d_row6_col2\" class=\"data row6 col2\" >0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1603d_level0_row7\" class=\"row_heading level0 row7\" >max</th>\n",
       "      <td id=\"T_1603d_row7_col0\" class=\"data row7 col0\" >14.77</td>\n",
       "      <td id=\"T_1603d_row7_col1\" class=\"data row7 col1\" >1.79</td>\n",
       "      <td id=\"T_1603d_row7_col2\" class=\"data row7 col2\" >1.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x240de715400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.describe().style.format(\"{0:.2f}\").set_caption(\"Before normalization :\"))\n",
    "\n",
    "mean = X_train.mean()\n",
    "std  = X_train.std()\n",
    "X_train = (X_train - mean) / std\n",
    "X_test  = (X_test  - mean) / std\n",
    "\n",
    "display(X_train.describe().style.format(\"{0:.2f}\").set_caption(\"After normalization :\"))\n",
    "\n",
    "# Convert ou DataFrame to numpy array\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_test,  y_test  = np.array(X_test),  np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5c4ee-5fef-4705-8fce-131825a45cde",
   "metadata": {},
   "source": [
    "# Build DNN model with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57ea4b38-9b10-4c45-b3e4-527155a2b5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_n1 (Dense)            (None, 64)                256       \n",
      "                                                                 \n",
      " Dense_n2 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model_v1(shape):\n",
    " model = keras.models.Sequential()\n",
    " model.add(keras.layers.Input(shape, name=\"InputLayer\"))\n",
    " model.add(keras.layers.Dense(64, activation='relu', name='Dense_n1'))\n",
    " model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))\n",
    " model.add(keras.layers.Dense(1, name='Output'))\n",
    " model.compile(optimizer='rmsprop', loss='mse', metrics=['mae','mse'])\n",
    " return model\n",
    "\n",
    "model=get_model_v1( (3,) )\n",
    "model.summary()\n",
    "os.makedirs('./run/models',   mode=0o750, exist_ok=True)\n",
    "save_dir = \"./run/models/best_model.h5\"\n",
    "savemodel_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir, verbose=0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd62ff-cf33-45c0-b772-82b30d4933e0",
   "metadata": {},
   "source": [
    "# Train and save the DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c86324ea-b808-467b-bbb0-48da44ea776f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 35.1963 - mae: 3.5276 - mse: 35.1963 - val_loss: 40.1775 - val_mae: 3.4428 - val_mse: 40.1775\n",
      "Epoch 2/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.8199 - mae: 3.4712 - mse: 32.8199 - val_loss: 38.7816 - val_mae: 3.7506 - val_mse: 38.7816\n",
      "Epoch 3/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.5212 - mae: 3.4580 - mse: 32.5212 - val_loss: 38.5120 - val_mae: 3.6987 - val_mse: 38.5120\n",
      "Epoch 4/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.4886 - mae: 3.4519 - mse: 32.4886 - val_loss: 39.6179 - val_mae: 3.3736 - val_mse: 39.6179\n",
      "Epoch 5/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3641 - mae: 3.4500 - mse: 32.3641 - val_loss: 38.4854 - val_mae: 3.5287 - val_mse: 38.4854\n",
      "Epoch 6/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.4019 - mae: 3.4485 - mse: 32.4019 - val_loss: 39.3278 - val_mae: 3.3750 - val_mse: 39.3278\n",
      "Epoch 7/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3850 - mae: 3.4479 - mse: 32.3850 - val_loss: 38.8575 - val_mae: 3.4483 - val_mse: 38.8575\n",
      "Epoch 8/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.4505 - mae: 3.4552 - mse: 32.4505 - val_loss: 39.5094 - val_mae: 3.3154 - val_mse: 39.5094\n",
      "Epoch 9/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.4154 - mae: 3.4402 - mse: 32.4154 - val_loss: 39.2173 - val_mae: 3.4667 - val_mse: 39.2173\n",
      "Epoch 10/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.4016 - mae: 3.4459 - mse: 32.4016 - val_loss: 39.3817 - val_mae: 3.4462 - val_mse: 39.3817\n",
      "Epoch 11/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.3237 - mae: 3.4456 - mse: 32.3237 - val_loss: 38.4120 - val_mae: 3.7165 - val_mse: 38.4120\n",
      "Epoch 12/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.3335 - mae: 3.4505 - mse: 32.3335 - val_loss: 38.4917 - val_mae: 3.5151 - val_mse: 38.4917\n",
      "Epoch 13/100\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 32.3202 - mae: 3.4401 - mse: 32.3202 - val_loss: 38.4289 - val_mae: 3.5194 - val_mse: 38.4289\n",
      "Epoch 14/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.3697 - mae: 3.4391 - mse: 32.3697 - val_loss: 39.2642 - val_mae: 3.3398 - val_mse: 39.2642\n",
      "Epoch 15/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.2824 - mae: 3.4353 - mse: 32.2824 - val_loss: 39.7117 - val_mae: 3.3579 - val_mse: 39.7117\n",
      "Epoch 16/100\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 32.2938 - mae: 3.4461 - mse: 32.2938 - val_loss: 39.2081 - val_mae: 3.6097 - val_mse: 39.2081\n",
      "Epoch 17/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3169 - mae: 3.4336 - mse: 32.3169 - val_loss: 38.5494 - val_mae: 3.7932 - val_mse: 38.5494\n",
      "Epoch 18/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2925 - mae: 3.4517 - mse: 32.2925 - val_loss: 38.6642 - val_mae: 3.6391 - val_mse: 38.6642\n",
      "Epoch 19/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3502 - mae: 3.4515 - mse: 32.3502 - val_loss: 38.5239 - val_mae: 3.7915 - val_mse: 38.5239\n",
      "Epoch 20/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.2399 - mae: 3.4414 - mse: 32.2399 - val_loss: 38.6275 - val_mae: 3.7744 - val_mse: 38.6275\n",
      "Epoch 21/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2422 - mae: 3.4395 - mse: 32.2422 - val_loss: 38.6294 - val_mae: 3.7583 - val_mse: 38.6294\n",
      "Epoch 22/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2742 - mae: 3.4471 - mse: 32.2742 - val_loss: 38.5225 - val_mae: 3.7865 - val_mse: 38.5225\n",
      "Epoch 23/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.3354 - mae: 3.4452 - mse: 32.3354 - val_loss: 38.9957 - val_mae: 3.3912 - val_mse: 38.9957\n",
      "Epoch 24/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2488 - mae: 3.4374 - mse: 32.2488 - val_loss: 38.9400 - val_mae: 3.5347 - val_mse: 38.9400\n",
      "Epoch 25/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2933 - mae: 3.4359 - mse: 32.2933 - val_loss: 39.6915 - val_mae: 3.4132 - val_mse: 39.6915\n",
      "Epoch 26/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2404 - mae: 3.4442 - mse: 32.2404 - val_loss: 39.1080 - val_mae: 3.4904 - val_mse: 39.1080\n",
      "Epoch 27/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2517 - mae: 3.4453 - mse: 32.2517 - val_loss: 39.5724 - val_mae: 3.3219 - val_mse: 39.5724\n",
      "Epoch 28/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1963 - mae: 3.4293 - mse: 32.1963 - val_loss: 38.4002 - val_mae: 3.6138 - val_mse: 38.4002\n",
      "Epoch 29/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.1745 - mae: 3.4338 - mse: 32.1745 - val_loss: 38.7342 - val_mae: 3.5285 - val_mse: 38.7342\n",
      "Epoch 30/100\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 32.1650 - mae: 3.4298 - mse: 32.1650 - val_loss: 39.3368 - val_mae: 4.1090 - val_mse: 39.3368\n",
      "Epoch 31/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2827 - mae: 3.4448 - mse: 32.2827 - val_loss: 38.3957 - val_mae: 3.5691 - val_mse: 38.3957\n",
      "Epoch 32/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2776 - mae: 3.4240 - mse: 32.2776 - val_loss: 38.6424 - val_mae: 3.7291 - val_mse: 38.6424\n",
      "Epoch 33/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2532 - mae: 3.4419 - mse: 32.2532 - val_loss: 39.3162 - val_mae: 3.4427 - val_mse: 39.3162\n",
      "Epoch 34/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.2602 - mae: 3.4313 - mse: 32.2602 - val_loss: 38.5163 - val_mae: 3.6652 - val_mse: 38.5163\n",
      "Epoch 35/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2237 - mae: 3.4442 - mse: 32.2237 - val_loss: 38.9088 - val_mae: 3.5798 - val_mse: 38.9088\n",
      "Epoch 36/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1014 - mae: 3.4263 - mse: 32.1014 - val_loss: 38.4419 - val_mae: 3.7468 - val_mse: 38.4419\n",
      "Epoch 37/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.1992 - mae: 3.4422 - mse: 32.1992 - val_loss: 38.9271 - val_mae: 3.4564 - val_mse: 38.9271\n",
      "Epoch 38/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2040 - mae: 3.4260 - mse: 32.2040 - val_loss: 38.5993 - val_mae: 3.7908 - val_mse: 38.5993\n",
      "Epoch 39/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.2439 - mae: 3.4376 - mse: 32.2439 - val_loss: 38.6811 - val_mae: 3.5028 - val_mse: 38.6811\n",
      "Epoch 40/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.2648 - mae: 3.4299 - mse: 32.2648 - val_loss: 38.8465 - val_mae: 3.4897 - val_mse: 38.8465\n",
      "Epoch 41/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.1920 - mae: 3.4336 - mse: 32.1920 - val_loss: 38.7668 - val_mae: 3.5509 - val_mse: 38.7668\n",
      "Epoch 42/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1258 - mae: 3.4219 - mse: 32.1258 - val_loss: 38.7052 - val_mae: 3.4955 - val_mse: 38.7052\n",
      "Epoch 43/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1617 - mae: 3.4173 - mse: 32.1617 - val_loss: 38.6717 - val_mae: 3.4871 - val_mse: 38.6717\n",
      "Epoch 44/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2106 - mae: 3.4188 - mse: 32.2106 - val_loss: 38.8588 - val_mae: 3.4279 - val_mse: 38.8588\n",
      "Epoch 45/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.0872 - mae: 3.4269 - mse: 32.0872 - val_loss: 39.0247 - val_mae: 3.5484 - val_mse: 39.0247\n",
      "Epoch 46/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1869 - mae: 3.4217 - mse: 32.1869 - val_loss: 38.6985 - val_mae: 3.7902 - val_mse: 38.6985\n",
      "Epoch 47/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1500 - mae: 3.4342 - mse: 32.1500 - val_loss: 38.7219 - val_mae: 3.4495 - val_mse: 38.7219\n",
      "Epoch 48/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1511 - mae: 3.4213 - mse: 32.1511 - val_loss: 38.7335 - val_mae: 3.8561 - val_mse: 38.7335\n",
      "Epoch 49/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1779 - mae: 3.4280 - mse: 32.1779 - val_loss: 38.7539 - val_mae: 3.7990 - val_mse: 38.7539\n",
      "Epoch 50/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1869 - mae: 3.4444 - mse: 32.1869 - val_loss: 38.5004 - val_mae: 3.6869 - val_mse: 38.5004\n",
      "Epoch 51/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1729 - mae: 3.4272 - mse: 32.1729 - val_loss: 38.4151 - val_mae: 3.5707 - val_mse: 38.4151\n",
      "Epoch 52/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1477 - mae: 3.4349 - mse: 32.1477 - val_loss: 38.6019 - val_mae: 3.8193 - val_mse: 38.6019\n",
      "Epoch 53/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1624 - mae: 3.4409 - mse: 32.1624 - val_loss: 38.7267 - val_mae: 3.7444 - val_mse: 38.7267\n",
      "Epoch 54/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1731 - mae: 3.4362 - mse: 32.1731 - val_loss: 38.7322 - val_mae: 3.7080 - val_mse: 38.7322\n",
      "Epoch 55/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2454 - mae: 3.4274 - mse: 32.2454 - val_loss: 39.0460 - val_mae: 3.3856 - val_mse: 39.0460\n",
      "Epoch 56/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1934 - mae: 3.4209 - mse: 32.1934 - val_loss: 39.7667 - val_mae: 3.3860 - val_mse: 39.7667\n",
      "Epoch 57/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2241 - mae: 3.4301 - mse: 32.2241 - val_loss: 39.7259 - val_mae: 3.3278 - val_mse: 39.7259\n",
      "Epoch 58/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1587 - mae: 3.4263 - mse: 32.1587 - val_loss: 39.7098 - val_mae: 3.3504 - val_mse: 39.7098\n",
      "Epoch 59/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1828 - mae: 3.4428 - mse: 32.1828 - val_loss: 39.0606 - val_mae: 3.3573 - val_mse: 39.0606\n",
      "Epoch 60/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2219 - mae: 3.4270 - mse: 32.2219 - val_loss: 38.9128 - val_mae: 3.4287 - val_mse: 38.9128\n",
      "Epoch 61/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2331 - mae: 3.4301 - mse: 32.2331 - val_loss: 38.7503 - val_mae: 3.4805 - val_mse: 38.7503\n",
      "Epoch 62/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2110 - mae: 3.4215 - mse: 32.2110 - val_loss: 38.9984 - val_mae: 3.7232 - val_mse: 38.9984\n",
      "Epoch 63/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2366 - mae: 3.4368 - mse: 32.2366 - val_loss: 38.7023 - val_mae: 3.4876 - val_mse: 38.7023\n",
      "Epoch 64/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1901 - mae: 3.4236 - mse: 32.1901 - val_loss: 39.0735 - val_mae: 3.7198 - val_mse: 39.0735\n",
      "Epoch 65/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1645 - mae: 3.4337 - mse: 32.1645 - val_loss: 40.1379 - val_mae: 3.3498 - val_mse: 40.1379\n",
      "Epoch 66/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2524 - mae: 3.4248 - mse: 32.2524 - val_loss: 39.0427 - val_mae: 3.5123 - val_mse: 39.0427\n",
      "Epoch 67/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2737 - mae: 3.4293 - mse: 32.2737 - val_loss: 38.6104 - val_mae: 3.6856 - val_mse: 38.6104\n",
      "Epoch 68/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1660 - mae: 3.4266 - mse: 32.1660 - val_loss: 39.9603 - val_mae: 3.3842 - val_mse: 39.9603\n",
      "Epoch 69/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1716 - mae: 3.4280 - mse: 32.1716 - val_loss: 38.8453 - val_mae: 3.7598 - val_mse: 38.8453\n",
      "Epoch 70/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2481 - mae: 3.4283 - mse: 32.2481 - val_loss: 38.9293 - val_mae: 3.7255 - val_mse: 38.9293\n",
      "Epoch 71/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2245 - mae: 3.4250 - mse: 32.2245 - val_loss: 38.7083 - val_mae: 3.7330 - val_mse: 38.7083\n",
      "Epoch 72/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1781 - mae: 3.4250 - mse: 32.1781 - val_loss: 38.9963 - val_mae: 3.5378 - val_mse: 38.9963\n",
      "Epoch 73/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1235 - mae: 3.4359 - mse: 32.1235 - val_loss: 39.7316 - val_mae: 3.2620 - val_mse: 39.7316\n",
      "Epoch 74/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1627 - mae: 3.4126 - mse: 32.1627 - val_loss: 39.2464 - val_mae: 3.5501 - val_mse: 39.2464\n",
      "Epoch 75/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1893 - mae: 3.4228 - mse: 32.1893 - val_loss: 38.9237 - val_mae: 3.8423 - val_mse: 38.9237\n",
      "Epoch 76/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1628 - mae: 3.4327 - mse: 32.1628 - val_loss: 38.9319 - val_mae: 3.5562 - val_mse: 38.9319\n",
      "Epoch 77/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2136 - mae: 3.4374 - mse: 32.2136 - val_loss: 39.8349 - val_mae: 3.2957 - val_mse: 39.8349\n",
      "Epoch 78/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2005 - mae: 3.4240 - mse: 32.2005 - val_loss: 38.9227 - val_mae: 3.5257 - val_mse: 38.9227\n",
      "Epoch 79/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.2294 - mae: 3.4206 - mse: 32.2294 - val_loss: 39.6239 - val_mae: 3.3722 - val_mse: 39.6239\n",
      "Epoch 80/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.1813 - mae: 3.4296 - mse: 32.1813 - val_loss: 40.2913 - val_mae: 3.3075 - val_mse: 40.2913\n",
      "Epoch 81/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.1864 - mae: 3.4250 - mse: 32.1864 - val_loss: 38.7408 - val_mae: 3.4995 - val_mse: 38.7408\n",
      "Epoch 82/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1327 - mae: 3.4162 - mse: 32.1327 - val_loss: 38.8826 - val_mae: 3.8493 - val_mse: 38.8826\n",
      "Epoch 83/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2100 - mae: 3.4260 - mse: 32.2100 - val_loss: 38.5533 - val_mae: 3.5751 - val_mse: 38.5533\n",
      "Epoch 84/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1921 - mae: 3.4247 - mse: 32.1921 - val_loss: 40.0804 - val_mae: 3.2705 - val_mse: 40.0804\n",
      "Epoch 85/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2734 - mae: 3.4295 - mse: 32.2734 - val_loss: 38.7252 - val_mae: 3.6044 - val_mse: 38.7252\n",
      "Epoch 86/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1517 - mae: 3.4300 - mse: 32.1517 - val_loss: 39.3509 - val_mae: 3.4716 - val_mse: 39.3509\n",
      "Epoch 87/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1506 - mae: 3.4197 - mse: 32.1506 - val_loss: 39.2165 - val_mae: 3.8938 - val_mse: 39.2165\n",
      "Epoch 88/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2154 - mae: 3.4259 - mse: 32.2154 - val_loss: 39.1185 - val_mae: 3.3886 - val_mse: 39.1185\n",
      "Epoch 89/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1515 - mae: 3.4222 - mse: 32.1515 - val_loss: 38.8316 - val_mae: 3.5015 - val_mse: 38.8316\n",
      "Epoch 90/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2080 - mae: 3.4419 - mse: 32.2080 - val_loss: 38.8638 - val_mae: 3.4606 - val_mse: 38.8638\n",
      "Epoch 91/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2072 - mae: 3.4282 - mse: 32.2072 - val_loss: 39.1789 - val_mae: 3.9056 - val_mse: 39.1789\n",
      "Epoch 92/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1262 - mae: 3.4264 - mse: 32.1262 - val_loss: 39.3927 - val_mae: 3.4645 - val_mse: 39.3927\n",
      "Epoch 93/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2637 - mae: 3.4156 - mse: 32.2637 - val_loss: 39.0510 - val_mae: 3.4407 - val_mse: 39.0510\n",
      "Epoch 94/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2387 - mae: 3.4136 - mse: 32.2387 - val_loss: 38.8401 - val_mae: 3.4910 - val_mse: 38.8401\n",
      "Epoch 95/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.2332 - mae: 3.4193 - mse: 32.2332 - val_loss: 38.4573 - val_mae: 3.5632 - val_mse: 38.4573\n",
      "Epoch 96/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1016 - mae: 3.4246 - mse: 32.1016 - val_loss: 38.4420 - val_mae: 3.7274 - val_mse: 38.4420\n",
      "Epoch 97/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1122 - mae: 3.4208 - mse: 32.1122 - val_loss: 38.9595 - val_mae: 3.5976 - val_mse: 38.9595\n",
      "Epoch 98/100\n",
      "819/819 [==============================] - 1s 2ms/step - loss: 32.1914 - mae: 3.4286 - mse: 32.1914 - val_loss: 38.8711 - val_mae: 3.5507 - val_mse: 38.8711\n",
      "Epoch 99/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.1597 - mae: 3.4191 - mse: 32.1597 - val_loss: 39.2349 - val_mae: 3.4051 - val_mse: 39.2349\n",
      "Epoch 100/100\n",
      "819/819 [==============================] - 2s 2ms/step - loss: 32.2472 - mae: 3.4250 - mse: 32.2472 - val_loss: 40.3687 - val_mae: 3.3113 - val_mse: 40.3687\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs= 100, batch_size= 10, validation_data = (X_test, y_test), callbacks= [savemodel_callback])\n",
    "model.save('my_model.h5')\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d782288-50f3-4094-b696-7a93d589f9cc",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d28eb09c-7978-476e-b164-c8602915819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test / loss      : 40.3687\n",
      "x_test / mae       : 3.3113\n",
      "x_test / mse       : 40.3687\n",
      "min( val_mae ) : 3.2620\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))\n",
    "print('x_test / mse       : {:5.4f}'.format(score[2]))\n",
    "print(\"min( val_mae ) : {:.4f}\".format( min(history.history[\"val_mae\"]) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470254ec-9c90-4ff8-ad8b-467fe7b8f615",
   "metadata": {},
   "source": [
    "# Make and display prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "408906e9-3fcf-428b-87f0-c456a3ef9b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 984us/step\n",
      "[[8.397333 ]\n",
      " [2.698166 ]\n",
      " [3.146985 ]\n",
      " ...\n",
      " [4.1457257]\n",
      " [7.6573205]\n",
      " [3.473034 ]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a83fc7f-d551-4257-951c-c742ee3405fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture    Prediction   Real   Delta\n",
      "000        8.40       32.97      +24.57 \n",
      "001        2.70       1.54      -1.16 \n",
      "002        3.15       2.5      -0.65 \n",
      "003        8.43       2.13      -6.30 \n",
      "004        4.86       1.7      -3.16 \n",
      "005        2.92       6.19      +3.27 \n",
      "006        4.93       1.71      -3.22 \n",
      "007        1.44       1.61      +0.17 \n",
      "008        3.90       1.85      -2.05 \n",
      "009        0.90       1.48      +0.58 \n",
      "010        4.16       1.37      -2.79 \n",
      "011        3.47       14.0      +10.53 \n",
      "012        4.10       2.19      -1.91 \n",
      "013        3.61       11.38      +7.77 \n",
      "014        6.56       12.24      +5.68 \n",
      "015        4.53       7.82      +3.29 \n",
      "016        1.86       1.37      -0.49 \n",
      "017        5.36       4.08      -1.28 \n",
      "018        2.16       2.19      +0.03 \n",
      "019        6.75       1.93      -4.82 \n",
      "020        2.42       1.54      -0.88 \n",
      "021        3.35       2.64      -0.71 \n",
      "022        1.87       1.57      -0.30 \n",
      "023        2.73       2.41      -0.32 \n",
      "024        3.92       6.85      +2.93 \n",
      "025        2.88       2.97      +0.09 \n",
      "026        4.71       5.1      +0.39 \n",
      "027        3.35       2.0      -1.35 \n",
      "028        2.60       1.97      -0.63 \n",
      "029        5.55       2.0      -3.55 \n",
      "030        5.13       1.44      -3.69 \n",
      "031        7.80       1.42      -6.38 \n",
      "032        4.08       3.71      -0.37 \n",
      "033        3.70       3.5      -0.20 \n",
      "034        8.06       7.98      -0.08 \n",
      "035        2.10       5.1      +3.00 \n",
      "036        1.60       1.59      -0.01 \n",
      "037        4.78       1.71      -3.07 \n",
      "038        4.13       20.19      +16.06 \n",
      "039        6.10       3.74      -2.36 \n",
      "040        8.12       22.44      +14.32 \n",
      "041        4.14       9.66      +5.52 \n",
      "042        3.14       2.1      -1.04 \n",
      "043        2.44       1.57      -0.87 \n",
      "044        0.62       1.66      +1.04 \n",
      "045        6.69       2.83      -3.86 \n",
      "046        2.75       5.3      +2.55 \n",
      "047        0.79       9.81      +9.02 \n",
      "048        2.16       5.1      +2.94 \n",
      "049        4.93       2.12      -2.81 \n",
      "050        2.71       1.83      -0.88 \n",
      "051        4.06       2.61      -1.45 \n",
      "052        2.03       1.56      -0.47 \n",
      "053        3.64       2.29      -1.35 \n",
      "054        3.94       5.11      +1.17 \n",
      "055        4.65       1.63      -3.02 \n",
      "056        7.64       6.96      -0.68 \n",
      "057        3.22       8.95      +5.73 \n",
      "058        4.04       10.9      +6.86 \n",
      "059        7.91       4.67      -3.24 \n",
      "060        4.90       2.2      -2.70 \n",
      "061        8.11       6.14      -1.97 \n",
      "062        4.06       6.91      +2.85 \n",
      "063        3.87       2.06      -1.81 \n",
      "064        6.87       1.62      -5.25 \n",
      "065        1.04       8.13      +7.09 \n",
      "066        3.50       2.36      -1.14 \n",
      "067        3.74       22.77      +19.03 \n",
      "068        5.03       1.33      -3.70 \n",
      "069        3.14       2.92      -0.22 \n",
      "070        4.14       1.3      -2.84 \n",
      "071        2.70       11.8      +9.10 \n",
      "072        6.33       2.65      -3.68 \n",
      "073        2.69       1.51      -1.18 \n",
      "074        3.15       1.42      -1.73 \n",
      "075        3.59       3.79      +0.20 \n",
      "076        0.98       1.47      +0.49 \n",
      "077        3.94       20.61      +16.67 \n",
      "078        5.35       4.62      -0.73 \n",
      "079        6.26       31.64      +25.38 \n",
      "080        3.96       4.08      +0.12 \n",
      "081        2.98       7.2      +4.22 \n",
      "082        4.39       5.6      +1.21 \n",
      "083        8.14       4.67      -3.47 \n",
      "084        3.12       2.1      -1.02 \n",
      "085        3.74       3.3      -0.44 \n",
      "086        3.32       2.33      -0.99 \n",
      "087        3.15       8.24      +5.09 \n",
      "088        2.95       2.92      -0.03 \n",
      "089        2.71       1.57      -1.14 \n",
      "090        5.51       1.73      -3.78 \n",
      "091        3.93       2.22      -1.71 \n",
      "092        5.83       2.2      -3.63 \n",
      "093        6.46       11.45      +4.99 \n",
      "094        3.37       1.31      -2.06 \n",
      "095        6.78       7.83      +1.05 \n",
      "096        3.04       2.85      -0.19 \n",
      "097        1.59       1.59      +0.00 \n",
      "098        1.73       1.58      -0.15 \n",
      "099        3.89       7.24      +3.35 \n",
      "100        2.25       3.82      +1.57 \n",
      "101        7.16       9.36      +2.20 \n",
      "102        5.59       2.67      -2.92 \n",
      "103        3.23       1.32      -1.91 \n",
      "104        4.86       4.3      -0.56 \n",
      "105        2.59       6.57      +3.98 \n",
      "106        0.62       2.15      +1.53 \n",
      "107        2.04       1.49      -0.55 \n",
      "108        7.80       2.28      -5.52 \n",
      "109        2.63       4.3      +1.67 \n",
      "110        4.12       4.96      +0.84 \n",
      "111        7.97       13.1      +5.13 \n",
      "112        4.41       3.12      -1.29 \n",
      "113        5.48       2.71      -2.77 \n",
      "114        5.52       13.48      +7.96 \n",
      "115        3.50       4.16      +0.66 \n",
      "116        5.86       2.0      -3.86 \n",
      "117        1.36       3.34      +1.98 \n",
      "118        3.87       5.26      +1.39 \n",
      "119        2.96       1.77      -1.19 \n",
      "120        3.66       4.07      +0.41 \n",
      "121        3.78       1.76      -2.02 \n",
      "122        1.18       1.64      +0.46 \n",
      "123        3.30       1.5      -1.80 \n",
      "124        3.15       2.1      -1.05 \n",
      "125        5.76       9.21      +3.45 \n",
      "126        3.97       2.23      -1.74 \n",
      "127        4.05       1.42      -2.63 \n",
      "128        2.68       2.75      +0.07 \n",
      "129        2.53       2.43      -0.10 \n",
      "130        4.03       3.8      -0.23 \n",
      "131        4.47       1.81      -2.66 \n",
      "132        5.06       6.54      +1.48 \n",
      "133        0.87       8.26      +7.39 \n",
      "134        0.52       2.15      +1.63 \n",
      "135        2.74       1.3      -1.44 \n",
      "136        3.76       2.5      -1.26 \n",
      "137        1.27       1.62      +0.35 \n",
      "138        7.40       5.0      -2.40 \n",
      "139        5.39       8.9      +3.51 \n",
      "140        3.59       4.83      +1.24 \n",
      "141        2.20       1.55      -0.65 \n",
      "142        2.06       5.99      +3.93 \n",
      "143        4.58       11.63      +7.05 \n",
      "144        4.07       2.85      -1.22 \n",
      "145        8.11       5.17      -2.94 \n",
      "146        3.26       1.9      -1.36 \n",
      "147        4.85       3.79      -1.06 \n",
      "148        4.34       16.7      +12.36 \n",
      "149        3.52       1.27      -2.25 \n",
      "150        2.73       2.68      -0.05 \n",
      "151        4.22       1.7      -2.52 \n",
      "152        7.81       17.18      +9.37 \n",
      "153        2.68       27.0      +24.32 \n",
      "154        0.42       3.48      +3.06 \n",
      "155        6.32       14.54      +8.22 \n",
      "156        3.43       2.5      -0.93 \n",
      "157        4.35       3.55      -0.80 \n",
      "158        4.47       2.1      -2.37 \n",
      "159        0.90       1.48      +0.58 \n",
      "160        4.63       1.7      -2.93 \n",
      "161        3.64       2.04      -1.60 \n",
      "162        2.71       1.82      -0.89 \n",
      "163        1.99       3.52      +1.53 \n",
      "164        7.68       15.1      +7.42 \n",
      "165        2.72       30.4      +27.68 \n",
      "166        5.11       10.22      +5.11 \n",
      "167        3.60       5.81      +2.21 \n",
      "168        3.47       1.2      -2.27 \n",
      "169        2.08       1.56      -0.52 \n",
      "170        4.16       2.28      -1.88 \n",
      "171        3.50       14.2      +10.70 \n",
      "172        3.48       11.11      +7.63 \n",
      "173        3.47       1.3      -2.17 \n",
      "174        2.71       1.5      -1.21 \n",
      "175        1.02       8.6      +7.58 \n",
      "176        7.65       12.68      +5.03 \n",
      "177        7.17       10.29      +3.12 \n",
      "178        0.84       1.48      +0.64 \n",
      "179        3.77       15.4      +11.63 \n",
      "180        3.04       1.79      -1.25 \n",
      "181        8.38       9.7      +1.32 \n",
      "182        2.73       2.3      -0.43 \n",
      "183        3.08       2.62      -0.46 \n",
      "184        5.56       10.9      +5.34 \n",
      "185        4.30       14.0      +9.70 \n",
      "186        5.62       26.12      +20.50 \n",
      "187        0.87       2.64      +1.77 \n",
      "188        3.73       3.24      -0.49 \n",
      "189        2.18       15.18      +13.00 \n",
      "190        4.02       8.37      +4.35 \n",
      "191        2.54       3.16      +0.62 \n",
      "192        2.90       6.29      +3.39 \n",
      "193        5.58       7.09      +1.51 \n",
      "194        2.71       1.56      -1.15 \n",
      "195        7.41       12.02      +4.61 \n",
      "196        2.11       2.2      +0.09 \n",
      "197        3.01       4.58      +1.57 \n",
      "198        2.84       1.49      -1.35 \n",
      "199        3.05       2.1      -0.95 \n"
     ]
    }
   ],
   "source": [
    "n = 200\n",
    "ii = np.random.randint(1,len(X_test),n)\n",
    "x_sample = X_test[ii]\n",
    "y_sample = y_test[ii]\n",
    "print('Capture    Prediction   Real   Delta')\n",
    "for i in range(n):\n",
    "    pred   = y_pred[i][0]\n",
    "    real   = y_test[i]\n",
    "    delta  = real-pred\n",
    "    print(f'{i:03d}        {pred:.2f}       {real}      {delta:+.2f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b521e3c-ed60-443e-8fb4-0e7199268836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
